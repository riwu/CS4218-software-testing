We introduce a new dataset with human judgments on pairs of words in sentential context and evaluate our model on it showing that our model outperforms competitive baselines and other neural language models
Information Retrieval (IR) models need to deal with two difficult issues vocabulary mismatch and term dependencies
Vocabulary mismatch corresponds to the difficulty of retrieving relevant documents that do not contain exact query terms but semantically related terms
Term dependencies refers to the need of considering the relationship between the words of the query when estimating the relevance of a document
A multitude of solutions has been proposed to solve each of these two problems but no principled model solve both
In parallel in the last few years language models based on neural networks have been used to cope with complex natural language processing tasks like emotion and paraphrase detection
Although they present good abilities to cope with both term dependencies and vocabulary mismatch problems thanks to the distributed representation of words they are based upon such models could not be used readily in IR where the estimation of one language model per document (or query) is required
This is both computationally unfeasible and prone to over-fitting